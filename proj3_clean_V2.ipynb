{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data from csv's if it has already been converted so such.  Else, ignore this line and start with converting osm -> csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(\"nodes.csv\")\n",
    "nodes_tags = pd.read_csv(\"nodes_tags.csv\")\n",
    "ways = pd.read_csv('ways.csv')\n",
    "ways_nodes = pd.read_csv(\"ways_nodes.csv\")\n",
    "ways_tags = pd.read_csv(\"ways_tags.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OpenStreemMap XML file to local file using web api overpass-api.de query form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"from data.py\n",
    "This will extract fields from the OpenStreetMap XML file and store various field values in ways and nodes CSV files.\n",
    "Parsing of field values includig ':' and separately problem characters (not a-z,:) is done.  The field vlaues that are \n",
    "extracted are included in the 'NODE_FIELDS','NODE_TAGS_PATH','WAYS_PATH','WAY_NODES_PATH','WAY_TAGS_PATH' lists.  The \n",
    "schema is validated as well.\n",
    "\n",
    "A small subset of the data was validated with the provided function and then validation set to False for larger data set.\n",
    "\"\"\"\n",
    "\n",
    "OSM_PATH = \"Phx_metro_smaller.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "#create a regex that is any lower case letters : any lower cases letters\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "#create a regex of the troublesome characters that will make a tag be ignored in parsing\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        return parse_node(element)\n",
    "            \n",
    "    elif element.tag == 'way':\n",
    "        return parse_way(element)\n",
    "  \n",
    "    \n",
    "def parse_node(element):\n",
    "    #Parse an element.tag = node component\n",
    "    node_attribs = {}\n",
    "    tags = []\n",
    "    for att in NODE_FIELDS:\n",
    "        node_attribs[att] = element.attrib[att]\n",
    "        \n",
    "    #build node_tags\n",
    "    position = 0\n",
    "    for child in element:\n",
    "        if child.tag == 'tag':\n",
    "            tag = parse_tag(element, child)\n",
    "            tags.append(tag)\n",
    "    return {'node': node_attribs, 'node_tags' : tags}    \n",
    "\n",
    "\n",
    "def parse_way(element):\n",
    "    #Parse an element.tag = way component\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []\n",
    "    #build top level way attributes\n",
    "    for att in WAY_FIELDS:\n",
    "        way_attribs[att] = element.attrib[att]\n",
    "    \n",
    "    #build way_tags\n",
    "    position = 0\n",
    "    for child in element:\n",
    "        if child.tag == 'tag':\n",
    "            tag = parse_tag(element, child)\n",
    "            tags.append(tag)\n",
    "        \n",
    "        if child.tag == 'nd':\n",
    "            #parse nd tag\n",
    "            nd, position = parse_nd(element, child, position)\n",
    "            way_nodes.append(nd)\n",
    "        \n",
    "    return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "def parse_nd(element, child, position):\n",
    "    #Parse component when child.tag = nd\n",
    "    nd = {}\n",
    "    nd['id'] = element.attrib['id']\n",
    "    nd['node_id'] = child.attrib['ref']\n",
    "    nd['position'] = position\n",
    "    position += 1\n",
    "    return nd, position\n",
    "\n",
    "def parse_tag(element, child):\n",
    "    #This parses both the way and node ``tag'' tag\n",
    "    tag = {}\n",
    "    tag['id'] = element.attrib['id']\n",
    "    \n",
    "    #parse k attrib with a colon or two\n",
    "    if bool(LOWER_COLON.search(child.attrib['k'])):\n",
    "        k = child.attrib['k'].split(':')\n",
    "        if len(k) == 3:\n",
    "            tag['key']=k[1]+':'+k[2]\n",
    "            \n",
    "        elif len(k) == 2:\n",
    "            tag['key'] = k[1]\n",
    "        tag['type']= k[0]\n",
    "        tag['value'] = child.attrib['v'] \n",
    "                \n",
    "    elif not bool(PROBLEMCHARS.search(child.attrib['k'])):\n",
    "        tag['type'] = 'regular'\n",
    "        tag['value']=child.attrib['v']\n",
    "        tag['key'] = child.attrib['k']\n",
    "            \n",
    "    return tag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def db_table_from_csv(db_name, file_name, table_name, keys = [], key_params = []):\n",
    "    '''file_name: name of csv file, \n",
    "    table: name of table to create in db_name, \n",
    "    keys: columsn of .csv file_name to include in db table\n",
    "    key_params: parameters for data type for each field in table'''\n",
    "\n",
    "    con = sqlite3.connect(db_name)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    #Creation of nodes ways table\n",
    "    cur.execute(\"DROP TABLE IF EXISTS \" + table_name + \";\")\n",
    "    con.commit()\n",
    "    \n",
    "    create_str = \"\" #holds schema string to pass to sql\n",
    "    for i in range(len(keys)):\n",
    "        create_str = create_str + keys[i] + \" \" + key_params[i]+\",\"\n",
    "    create_str = create_str[:-1]\n",
    "    cur.execute(\"CREATE TABLE \" + table_name + \"(\"+create_str+\");\")\n",
    "\n",
    "    con.commit()\n",
    "    with open(file_name, 'rb') as fin:\n",
    "        dr = csv.DictReader(fin)\n",
    "        to_db = [[i[key].decode(\"utf-8\") for key in keys] for i in dr]\n",
    "    \n",
    "    key_str = \"\"\n",
    "    key_qs = \"\"\n",
    "    for key in keys:\n",
    "        key_str = key_str+key+\",\"\n",
    "        key_qs = key_qs + \"?,\" #just how many fields to enter into db\n",
    "    key_qs = key_qs[:-1] #strip comma at end\n",
    "    key_str=key_str[:-1]\n",
    "    \n",
    "    #insert data into db according to keys\n",
    "    cur.executemany(\"INSERT INTO \"+table_name+\"(\"+key_str+\") VALUES (\"+key_qs+\");\", to_db)\n",
    "    con.commit()\n",
    "\n",
    "    con.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DB_NAME = 'Phx_metro_smaller.db'\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "NODE_PARAMS = ['INTEGER PRIMARY KEY NOT NULL', 'REAL', 'REAL', 'TEXT', 'INTEGER', 'TEXT', 'INTEGER', 'DATE']\n",
    "NODE_TAGS_PARAMS = ['INTEGER NOT NULL', 'TEXT NOT NULL', 'TEXT NOT NULL', 'TEXT NOT NULL']\n",
    "WAY_PARAMS = ['INTEGER NOT NULL', 'TEXT NOT NULL', 'INTEGER NOT NULL', 'TEXT NOT NULL', 'INTEGER NOT NULL', 'TEXT NOT NULL']\n",
    "WAY_NODES_PARAMS = ['INTEGER NOT NULL', 'INTEGER NOT NULL', 'INTEGER NOT NULL']\n",
    "WAY_TAGS_PARAMS = ['INTEGER NOT NULL', 'TEXT NOT NULL', 'TEXT NOT NULL', 'TEXT NOT NULL']\n",
    "\n",
    "\n",
    "db_table_from_csv(DB_NAME, 'nodes.csv', 'nodes', NODE_FIELDS, NODE_PARAMS)\n",
    "db_table_from_csv(DB_NAME, 'nodes_tags.csv', 'nodes_tags', NODE_TAGS_FIELDS, NODE_TAGS_PARAMS)\n",
    "db_table_from_csv(DB_NAME, 'ways.csv', 'ways', WAY_FIELDS, WAY_PARAMS)\n",
    "db_table_from_csv(DB_NAME, 'ways_tags.csv', 'way_tags', WAY_TAGS_FIELDS, WAY_TAGS_PARAMS)\n",
    "db_table_from_csv(DB_NAME, 'ways_nodes.csv', 'way_nodes', WAY_NODES_FIELDS, WAY_NODES_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cities list from http://phoenix.about.com/od/govtcity/qt/cities-towns-maricopa-county.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cities = ['Apache Junction','Avondale','Buckeye','Carefree','Cave Creek','Chandler','El Mirage','Fountain Hills',\n",
    "          'Gila Bend','Gilbert','Glendale','Goodyear','Guadalupe','Litchfield Park','Mesa','Paradise Valley',\n",
    "          'Peoria','Phoenix','Queen Creek','Scottsdale','Surprise','Tempe','Tolleson','Wickenburg','Youngtown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a measure, just check to see how close the word distance is among city/town names themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Apache Junction', ('', 0))\n",
      "('Avondale', ('Apache Junction', 0.2608695652173913))\n",
      "('Buckeye', ('Apache Junction', 0.18181818181818182))\n",
      "('Carefree', ('Buckeye', 0.4))\n",
      "('Cave Creek', ('Carefree', 0.6666666666666666))\n",
      "('Chandler', ('Avondale', 0.625))\n",
      "('El Mirage', ('Cave Creek', 0.42105263157894735))\n",
      "('Fountain Hills', ('Avondale', 0.36363636363636365))\n",
      "('Gila Bend', ('Avondale', 0.35294117647058826))\n",
      "('Gilbert', ('Gila Bend', 0.625))\n",
      "('Glendale', ('Avondale', 0.625))\n",
      "('Goodyear', ('Gilbert', 0.4))\n",
      "('Guadalupe', ('Avondale', 0.5882352941176471))\n",
      "('Litchfield Park', ('Cave Creek', 0.4))\n",
      "('Mesa', ('Glendale', 0.3333333333333333))\n",
      "('Paradise Valley', ('Avondale', 0.43478260869565216))\n",
      "('Peoria', ('El Mirage', 0.4))\n",
      "('Phoenix', ('Peoria', 0.46153846153846156))\n",
      "('Queen Creek', ('Cave Creek', 0.6666666666666666))\n",
      "('Scottsdale', ('Avondale', 0.5555555555555556))\n",
      "('Surprise', ('Peoria', 0.42857142857142855))\n",
      "('Tempe', ('El Mirage', 0.42857142857142855))\n",
      "('Tolleson', ('Phoenix', 0.4))\n",
      "('Wickenburg', ('Buckeye', 0.35294117647058826))\n",
      "('Youngtown', ('Fountain Hills', 0.43478260869565216))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cities)):\n",
    "    print(cities[i],best_match(cities[i],cities[:i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_match(text, match_list, case = False):\n",
    "    '''Matches text to match_list by finding the most similar element of match_list and returning\n",
    "    the element along with the score from 0 to 1 of how good the match is.  The case keyword allows one\n",
    "    to specify if the matching should be case sensitive or not, default is case=False so that case\n",
    "    is ignored'''\n",
    "    \n",
    "    from difflib import SequenceMatcher\n",
    "    max_match = 0\n",
    "    max_pos = 0\n",
    "    best_match = ''\n",
    "    for compare in match_list:\n",
    "        if case == False:\n",
    "            match_score = SequenceMatcher(None, text.lower(), compare.lower()).ratio()\n",
    "        elif case == True:\n",
    "            match_score = SequenceMatcher(None, text, compare).ratio()\n",
    "        if match_score > max_match:\n",
    "            max_match = match_score\n",
    "            best_match = compare\n",
    "        else:\n",
    "            continue\n",
    "    max_score = max_match\n",
    "    return (best_match, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and implementing the best_match function on the csv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for city_name in city_data_vals:\n",
    "    match,score = best_match(city_name, cities)\n",
    "    if score > 0.6:\n",
    "        print(\"Match {} to {}\".format(city_name, match))\n",
    "    else:\n",
    "        print(\"No match found for {}\".format(city_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now to run the csv data and send out to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unmatched = []\n",
    "for i in range(len(ways_tags)):\n",
    "    if ways_tags['key'][i] == 'city':\n",
    "        match, score = best_match(ways_tags['value'][i], cities, case=False)\n",
    "        if score > 0.6:\n",
    "            ways_tags.loc[i,'value'] = match\n",
    "        else:\n",
    "            unmatched.append(ways_tags['value'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fort McDowell',\n",
       " 'Gold Canyon',\n",
       " 'Higley',\n",
       " 'Laveen',\n",
       " 'Laveen Village',\n",
       " 'Luke AFB',\n",
       " 'Luke AFB, Waddell',\n",
       " 'Mayer',\n",
       " 'Morristown',\n",
       " 'Rio Verde',\n",
       " 'San Tan Valley',\n",
       " 'Sun City',\n",
       " 'Sun City West',\n",
       " 'Sun Lakes',\n",
       " 'Superstition Mountain',\n",
       " 'sun City West'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unmatched = []\n",
    "for i in range(len(nodes_tags)):\n",
    "    if nodes_tags['key'][i] == 'city':\n",
    "        match, score = best_match(nodes_tags['value'][i], cities, case=False)\n",
    "        if score > 0.6:\n",
    "            nodes_tags.loc[i,'value'] = match\n",
    "        else:\n",
    "            unmatched.append(nodes_tags['value'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198777</th>\n",
       "      <td>198777</td>\n",
       "      <td>198777</td>\n",
       "      <td>5618435</td>\n",
       "      <td>name_base</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>tiger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1       id        key      value   type county\n",
       "198777      198777        198777  5618435  name_base  San Diego  tiger    NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ways_tags[ways_tags['value'] == \"San Diego\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now manipulate the 'Street' vs 'St' type inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "CONVENTIONS = [['West','W'],['North','N'],[\"South\",\"S\"],['East','E'],['Street',\"St\"],[\"Road\",\"Rd\"],[\"Avenue\",\"Av\",\"Ave\"],\n",
    "                            [\"Place\",\"Pl\"],[\"Boulevard\",\"Blvd\"],[\"Trail\",\"Tr\"],[\"Place\",\"Pl\"],[\"Highway\",\"Hwy\",\"Hw\",\"Hy\"],\n",
    "                            [\"Parkway\",\"Pkwy\",\"Pw\"]]\n",
    "num_conventions = len(CONVENTIONS)\n",
    "\n",
    "def conv_street(street_name):\n",
    "    '''Convert abbreviations in the street names to their full word counter part as specified by CONVENTIONS'''\n",
    "    split_up = street_name.split(\" \")\n",
    "    for i in range(len(split_up)):\n",
    "        for j in range(num_conventions):\n",
    "            #Strip punctuation and compare to naming conventions list\n",
    "            if split_up[i].translate(None, string.punctuation) in CONVENTIONS[j]:\n",
    "                split_up[i] = CONVENTIONS[j][0]\n",
    "    #piece the street string back together with the change\n",
    "    whole = ' '.join(c for c in split_up)\n",
    "    return whole\n",
    "\n",
    "def conv_df(df):\n",
    "    #This function takes a pandas dataframe which contains a 'key' column with 'street' values and \n",
    "    # converts the values according to the conv_street function\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i,'key'] == 'street':\n",
    "            df.loc[i,'value'] = conv_street(df.loc[i,'value'])\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_tags = conv_df(nodes_tags)\n",
    "ways_tags = conv_df(ways_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create uniformity in the 'key' = 'state' values that refer to an address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AZ_reps = ['A', 'AS', 'AZ', 'AZ (Arizona)', 'AZZ', 'Arizona', 'Az', 'az']\n",
    "for i in range(len(ways_tags)):\n",
    "    if ways_tags.loc[i,'key'] == 'state' and ways_tags.loc[i,'type'] == 'addr' and ways_tags.loc[i,'value'] in AZ_reps:\n",
    "        ways_tags.loc[i,'value'] = 'AZ'\n",
    "\n",
    "        \n",
    "for i in range(len(nodes_tags)):\n",
    "    if nodes_tags.loc[i,'key'] == 'state' and nodes_tags.loc[i,'type'] == 'addr' and nodes_tags.loc[i,'value'] in AZ_reps:\n",
    "        nodes_tags.loc[i,'value'] = 'AZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check for what inconsistencies may exist in the 'key'='county' data in *_tags data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Maricopa'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(nodes_tags[nodes_tags['key'] == 'county']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gila, AZ',\n",
       " 'Maricopa',\n",
       " 'Maricopa, AZ',\n",
       " 'Maricopa, AZ:Yavapai, AZ',\n",
       " 'Pinal, AZ'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ways_tags[ways_tags['key'] == 'county']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will be easy enought to convert to just contain the county name and not the state abbreviation which should  be in another field for a database.  It will just take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(ways_tags)):\n",
    "    if ways_tags.loc[i,'key'] == 'county':\n",
    "        split = ways_tags.loc[i,'value'].split(\",\")\n",
    "        if 'Maricopa' == split[0]:\n",
    "            ways_tags.loc[i,'county'] = 'Maricopa'\n",
    "        elif 'Gila' == split[0]:\n",
    "            ways_tags.loc[i,'county'] = 'Gila'\n",
    "        elif 'Pinal' == split[0]:\n",
    "            ways_tags.loc[i,'county'] = 'Pinal'       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New since first submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def unif_postcodes(code):\n",
    "    #Function parses code from variations #####, #####-####, AZ ##### and will be converted to 5 digit only #####\n",
    "    code = code.split('-')[0]\n",
    "    if len(code.split(' ')) == 2:\n",
    "        return code.split(' ')[1]\n",
    "    else:\n",
    "        return code\n",
    "    \n",
    "##TESTING\n",
    "#print('44444',unif_postcodes('44444'))\n",
    "#print('AZ 44444',unif_postcodes('AZ 44444'))\n",
    "#print('44444-4444',unif_postcodes('44444-4444'))\n",
    "#print('AZ 44444-4444',unif_postcodes('AZ 44444-4444'))\n",
    "\n",
    "def process_df(df):\n",
    "    #Apply unif_postcode to a dataframe with particular columns\n",
    "    for i in range(len(df)):\n",
    "        if df['key'][i] == 'postcode':\n",
    "            df.loc[i,'value'] = unif_postcodes(df.loc[i,'value'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ways_tags = process_df(ways_tags)\n",
    "nodes_tags = process_df(nodes_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_tags.to_csv(\"nodes_tags.csv\")\n",
    "ways_tags.to_csv(\"ways_tags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create the OSM File to turn in with project\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"Phx_metro.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"Phx_metro_smaller.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
